{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLearning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3g7u60ZSS5H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RctUr3mtsUeb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Km8oFSK4p1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 30\n",
        "HIDEN_STATE = 256\n",
        "FILTER_SIZE = 150\n",
        "KERNEL_SIZE = 3\n",
        "embedding_dim=300\n",
        "glove_path = 'drive/My Drive/glove.42B.300d.txt'\n",
        "data_path = 'drive/My Drive/labeled_data.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngUdEaky9hZd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCNNEPpg_6ed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "def _get_data():\n",
        "        try:\n",
        "            data = pd.read_csv(data_path, sep=',', header=None, error_bad_lines=False) # use pandas to read CSV\n",
        "            data = data.dropna() # drop any rows with nans\n",
        "            Y = data[5]     #label of tweet\n",
        "           \n",
        "            return data[6].values, Y.values\n",
        "\n",
        "        except ValueError:\n",
        "            return None, None\n",
        "\n",
        "text = _get_data()[0][0]\n",
        "for t in _get_data()[0][1:]:\n",
        "  text += \" \"+ str(t)\n",
        "\n",
        "# lower max_font_size, change the maximum number of word and lighten the background:\n",
        "wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text)\n",
        "plt.figure()\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfp8rYAxP3-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.externals import joblib\n",
        "from sklearn import preprocessing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import pdb\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn import pipeline, feature_extraction, svm, metrics\n",
        "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import random\n",
        "  \n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn import pipeline, feature_extraction, svm, metrics\n",
        "from keras.layers import Bidirectional\n",
        "try:\n",
        "  from keras_self_attention import SeqSelfAttention\n",
        "except:\n",
        "  !pip install keras-self-attention\n",
        "  from keras_self_attention import SeqSelfAttention\n",
        "\n",
        "\n",
        "\n",
        "def _get_model( vocabulary_size=1, input_length=1, load_weights=False):\n",
        "    try:\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(vocabulary_size, 16, input_length=input_length))\n",
        "        model.add(MaxPooling1D(5))\n",
        "        model.add(LSTM(10))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n",
        "        return model\n",
        "    except ValueError:\n",
        "        return None\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MHrR2Rm6z9n",
        "colab_type": "text"
      },
      "source": [
        "Data preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVvFQSbS6sXJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "TAG_RE = re.compile(r'<[^>]+>')\n",
        "\n",
        "def _get_vocabulary(documents):\n",
        "    return set(\" \".join(documents).split())\n",
        "\n",
        "def _get_tokenizer(documents):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(documents)\n",
        "    return tokenizer\n",
        "\n",
        "def _clean_documents(documents):\n",
        "    # convert to list\n",
        "    documents = documents.tolist()\n",
        "    # clean docs\n",
        "    print(\"Starting cleaning %s documents\" % len(documents))\n",
        "    documents = [_clean_doc(doc) for doc in documents]\n",
        "    return documents\n",
        "\n",
        "def _clean_doc( doc):\n",
        "    # remove HTML tags\n",
        "    doc = _clean_tags(doc)\n",
        "    # replace all newlines and tabs\n",
        "    doc = doc.replace('\\\\n', ' ').replace('\\\\r', ' ').replace('\\\\t', ' ')\n",
        "    # add missing space after full stops and commas\n",
        "    doc = re.sub(r'(?<=[.,])(?=[^\\s])', r' ', doc)\n",
        "    \n",
        "    # create tokens\n",
        "    tokens = word_tokenize(doc)\n",
        "    # downcase\n",
        "    tokens = [w.lower() for w in tokens]\n",
        "    # regexp\n",
        "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    stripped = [re_punc.sub('', w) for w in tokens]\n",
        "    # remove non-alphabetic tokens\n",
        "    words = [word for word in stripped if word.isalpha()]\n",
        "    print('.', end='', flush=True)\n",
        "    return \" \".join(words)\n",
        "\n",
        "def _clean_tags( text):\n",
        "    return TAG_RE.sub('', text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UewoBWf8A-y",
        "colab_type": "text"
      },
      "source": [
        "Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbSSGCQJArUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data preprocessing\n",
        "try:\n",
        "  X, Y = _get_data()\n",
        "  documents = _clean_documents(X)\n",
        "except:\n",
        "  nltk.download(\"stopwords\")\n",
        "  nltk.download('punkt')\n",
        "  X, Y = _get_data()\n",
        "  documents = _clean_documents(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elKnSIXtAuEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "vocabulary = _get_vocabulary(documents)\n",
        "vocab_size = len(vocabulary) + 1\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(documents)\n",
        "\n",
        "word_index = tokenizer.word_index   #a dictionary with word as key and index as value\n",
        "\n",
        "word_value = dict([(v,k) for k, v in word_index.items()]) # a dictionary with index as key and word as value\n",
        "\n",
        "tokenizer = _get_tokenizer(documents)\n",
        "max_document_length = max([len(s.split()) for s in documents])\n",
        "encoded = tokenizer.texts_to_sequences(documents)\n",
        "X = sequence.pad_sequences(encoded, maxlen=max_document_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Excr6LulXVby",
        "colab_type": "text"
      },
      "source": [
        "Read GLOVE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLxrYCutoEYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "MAX_NUM_WORDS = 20000\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "#read glove embedding from my google drive\n",
        "embeddings_index = {}\n",
        "f = open(glove_path)\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTsIFGDr2i4B",
        "colab_type": "text"
      },
      "source": [
        "This method is used to Initialize glove embedding matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHW90NEkdnLv",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObZhJJJR2e56",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_embedding_matrix():\n",
        "  embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
        "  for word, i in word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "          # words not found in embedding index will be all-zeros.\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "      else: embedding_matrix[i] = random.random()\n",
        "  return embedding_matrix\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnUJjf_88KQH",
        "colab_type": "text"
      },
      "source": [
        "This function is used to initialize LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLQxilUH4pdX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import *\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "def LSTM_model():\n",
        "        #initialize embedding matrix\n",
        "        embedding_matrix = init_embedding_matrix()\n",
        "        #create and add layers to LSTM modle\n",
        "        model = Sequential()\n",
        "        #add embedding layer for my modle i use pretrained glove300d \n",
        "        model.add(Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_document_length, trainable=True))\n",
        "        #randomly drop some features to avoid overfitting\n",
        "        model.add(Dropout(0.5))   \n",
        "        #add lstm layer\n",
        "        model.add(LSTM(HIDEN_STATE, return_sequences = True))\n",
        "        #add attention layer\n",
        "        model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
        "        #flatten\n",
        "        model.add(GlobalAveragePooling1D())\n",
        "        #end make out put of 3 categories\n",
        "        model.add(Dense(3, activation=\"softmax\"))\n",
        "        #compile with adam\n",
        "        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "        return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOqxRKBQWp_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn import pipeline, feature_extraction, svm, metrics\n",
        "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
        "try:\n",
        "  from keras_self_attention import SeqSelfAttention\n",
        "except:\n",
        "  !pip install keras-self-attention\n",
        "  from keras_self_attention import SeqSelfAttention\n",
        "def CNN_model():\n",
        "\n",
        "        #initialize embedding matrix\n",
        "        embedding_matrix = init_embedding_matrix()\n",
        "        #create and add layers to LSTM modle\n",
        "        model = Sequential()\n",
        "        #add embedding layer for my modle i use pretrained glove300d \n",
        "        model.add(Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_document_length, trainable=True))\n",
        "        #randomly drop some features to avoid overfitting\n",
        "        model.add(Dropout(0.5))   \n",
        "        #add lstm layer\n",
        "        model.add(Conv1D(filters=FILTER_SIZE, kernel_size=KERNEL_SIZE, padding='same', activation='relu'))\n",
        "        #add attention layer\n",
        "        model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
        "        #flatten\n",
        "        model.add(GlobalAveragePooling1D())\n",
        "        #end make out put of 3 categories\n",
        "        model.add(Dense(3, activation=\"softmax\"))\n",
        "        #compile with adam\n",
        "        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "        return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHdooeGFpFKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classify(model, X, y):\n",
        "    # self.split(X,y)\n",
        "    cv = StratifiedKFold(n_splits=5)\n",
        "    results = [0,0,0, 0,0,0,0,0,0]\n",
        "    for train_idx, test_idx, in cv.split(X, y):\n",
        "        X_train, y_train = X[train_idx], y[train_idx]\n",
        "        X_test, y_test = X[test_idx], y[test_idx]\n",
        "        model.fit(X_train, y_train, validation_split=0.25, epochs=10, batch_size=BATCH_SIZE, verbose=1)\n",
        "\n",
        "        names = [weight.name for layer in model.layers for weight in layer.weights]\n",
        "        weights = model.get_weights()\n",
        "\n",
        "        y_pred = model.predict_classes(X_test)\n",
        "\n",
        "        y_test = [int(i) for i in y_test]\n",
        "\n",
        "        results = [x + y for x, y in zip(results , list( metrics.precision_recall_fscore_support(y_test, y_pred, average=\"macro\")[:-1] + metrics.precision_recall_fscore_support(y_test, y_pred, average=\"weighted\")[:-1]))]\n",
        "        \n",
        "    print([round(x/5 , 2) for x in results ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RCaXMl347k3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow\n",
        "LSTM_model = LSTM_model()\n",
        "print(\"starting lstm\")\n",
        "result = classify(LSTM_model, X, Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUiH0VGnynSg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CNN_model = CNN_model()\n",
        "print(\"starting CNN\")\n",
        "classify(CNN_model, X, Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcqImsnaOVCf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#check layers and weight of CNN model you can also modify it to check weights for lasm model\n",
        "for layer in CNN_model.layers:\n",
        "  g=layer.name\n",
        "  h=layer.get_weights()\n",
        "  print (g)\n",
        "  try:\n",
        "   print (h)\n",
        "  except:\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52zULxP0cYNl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfdwcNT7cPTA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNh7oYte_lEK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg12Z2d25IPM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpUcPeM0V-JW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}