{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLearning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCtjIgYbPoNH",
        "colab_type": "code",
        "outputId": "410d97f6-de4a-46ef-e07a-4ed0313433c7",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload() "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4f3236b4-0430-4506-9879-dc7c41d0db68\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-4f3236b4-0430-4506-9879-dc7c41d0db68\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving labeled_data.csv to labeled_data (1).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCNNEPpg_6ed",
        "colab_type": "code",
        "outputId": "7a8b90cc-377f-417d-ec72-670562ba5f8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "import pandas as pd\n",
        "def _get_data():\n",
        "        try:\n",
        "\n",
        "            # csv_path =  'labeled_data.csv'\n",
        "            # df = pd.read_csv(io.BytesIO(uploaded['labeled_data.csv']))\n",
        "            data = pd.read_csv('labeled_data.csv', sep=',', header=None, error_bad_lines=False) # use pandas to read CSV\n",
        "            data = data.dropna() # drop any rows with nans\n",
        "            Y = data[5]\n",
        "           \n",
        "            # Y = [int(i) for i in Y.values]\n",
        "            return data[6].values,Y.values\n",
        "        except ValueError:\n",
        "            return None, None\n",
        "_get_data()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([\"!!! RT @mayasolovely: As a woman you shouldn't complain about cleaning up your house. &amp; as a man you should always take the trash out...\",\n",
              "        '!!!!! RT @mleew17: boy dats cold...tyga dwn bad for cuffin dat hoe in the 1st place!!',\n",
              "        '!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby4life: You ever fuck a bitch and she start to cry? You be confused as shit',\n",
              "        ...,\n",
              "        'young buck wanna eat!!.. dat nigguh like I aint fuckin dis up again',\n",
              "        'youu got wild bitches tellin you lies',\n",
              "        '~~Ruffled | Ntac Eileen Dahlia - Beautiful color combination of pink, orange, yellow &amp; white. A Coll http://t.co/H0dYEBvnZB'],\n",
              "       dtype=object), array(['2', '1', '1', ..., '1', '1', '2'], dtype=object))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfp8rYAxP3-e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "1ed84533-d709-4fd5-8ca4-9762dbb13b6b"
      },
      "source": [
        "import io\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.externals import joblib\n",
        "from sklearn import preprocessing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import pdb\n",
        "\n",
        "\n",
        "import nltk\n",
        "\n",
        "\n",
        "\n",
        "TAG_RE = re.compile(r'<[^>]+>')\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 30\n",
        "\n",
        "\n",
        "def _get_vocabulary(documents):\n",
        "    return set(\" \".join(documents).split())\n",
        "\n",
        "def _get_tokenizer(documents):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(documents)\n",
        "    return tokenizer\n",
        "\n",
        "def _clean_documents(documents):\n",
        "    # convert to list\n",
        "    documents = documents.tolist()\n",
        "    # clean docs\n",
        "    print(\"Starting cleaning %s documents\" % len(documents))\n",
        "    documents = [_clean_doc(doc) for doc in documents]\n",
        "    return documents\n",
        "\n",
        "def _clean_doc( doc):\n",
        "    # remove HTML tags\n",
        "    doc = _clean_tags(doc)\n",
        "    # replace all newlines and tabs\n",
        "    doc = doc.replace('\\\\n', ' ').replace('\\\\r', ' ').replace('\\\\t', ' ')\n",
        "    # add missing space after full stops and commas\n",
        "    doc = re.sub(r'(?<=[.,])(?=[^\\s])', r' ', doc)\n",
        "    # create tokens\n",
        "    tokens = word_tokenize(doc)\n",
        "    # downcase\n",
        "    tokens = [w.lower() for w in tokens]\n",
        "    # regexp\n",
        "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    stripped = [re_punc.sub('', w) for w in tokens]\n",
        "    # remove non-alphabetic tokens\n",
        "    words = [word for word in stripped if word.isalpha()]\n",
        "    # remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [w for w in words if not w in stop_words]\n",
        "    # reduce word to base\n",
        "    porter = PorterStemmer()\n",
        "    stemmed = [porter.stem(word) for word in words]\n",
        "    print('.', end='', flush=True)\n",
        "    return \" \".join(stemmed)\n",
        "\n",
        "def _clean_tags( text):\n",
        "    return TAG_RE.sub('', text)\n",
        "\n",
        "def _get_model( vocabulary_size=1, input_length=1, load_weights=False):\n",
        "    try:\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(vocabulary_size, 16, input_length=input_length))\n",
        "        # model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\n",
        "        # model.add(Dropout(0.7))\n",
        "        model.add(MaxPooling1D(5))\n",
        "        model.add(LSTM(10))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n",
        "        return model\n",
        "    except ValueError:\n",
        "        return None\n",
        "\n",
        "# def _get_data():\n",
        "#     try:\n",
        "#         X = pd.read_csv(io.BytesIO(uploaded['labeled_data.csv']), sep=',', header=None, error_bad_lines=False) # use pandas to read CSV\n",
        "#         X = X.dropna() # drop any rows with nans\n",
        "\n",
        "#         return X[6].values,X[5].values\n",
        "#     except ValueError:\n",
        "#         return None, None\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIh1QG5eAlsE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUbJjyh34OsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbSSGCQJArUb",
        "colab_type": "code",
        "outputId": "1c3ae1ff-8f93-4373-e033-0b06186f9d3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "try:\n",
        "  X, Y = _get_data()\n",
        "  documents = _clean_documents(X)\n",
        "except:\n",
        "  nltk.download(\"stopwords\")\n",
        "  nltk.download('punkt')\n",
        "  X, Y = _get_data()\n",
        "  documents = _clean_documents(X)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting cleaning 24783 documents\n",
            "..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elKnSIXtAuEg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "49625536-60cf-4fae-f420-c8db51d85a40"
      },
      "source": [
        "vocabulary = _get_vocabulary(documents)\n",
        "vocab_size = len(vocabulary) + 1\n",
        "tokenizer = _get_tokenizer(documents)\n",
        "max_document_length = max([len(s.split()) for s in documents])\n",
        "encoded = tokenizer.texts_to_sequences(documents)\n",
        "X = sequence.pad_sequences(encoded, maxlen=max_document_length)\n",
        "model = _get_model(vocab_size, max_document_length)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLQxilUH4pdX",
        "colab_type": "code",
        "outputId": "b9b8da8d-ecf4-4434-f57b-8fa38b368b05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn import pipeline, feature_extraction, svm, metrics\n",
        "\n",
        "def LSTM_model():\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(vocab_size, 16, input_length=max_document_length))\n",
        "        # model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\n",
        "        model.add(Dropout(0.2))\n",
        "        # model.add(MaxPooling1D(5))\n",
        "        # model.add(LSTM(10, return_sequences = True))\n",
        "        # model.add(Dropout(0.5, ))\n",
        "        model.add(LSTM(10, return_sequences = True))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(LSTM(10))\n",
        "        model.add(Dropout(0.2))\n",
        "        # model.add(Dense(3, activation='softmax'))\n",
        "        # model.add(Dropout(0.5))\n",
        "        model.add(Dense(3, activation=\"softmax\"))\n",
        "        # model.add(Activation('softmax'))\n",
        "        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "def classify(X, y):\n",
        "    # self.split(X,y)\n",
        "    cv = StratifiedKFold(n_splits=5)\n",
        "    results = [0,0,0, 0,0,0,0,0,0]\n",
        "    for train_idx, test_idx, in cv.split(X, y):\n",
        "        X_train, y_train = X[train_idx], y[train_idx]\n",
        "        X_test, y_test = X[test_idx], y[test_idx]\n",
        "        model = LSTM_model()\n",
        "        model.fit(X_train, y_train, validation_split=0.2, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1)\n",
        "\n",
        "        y_pred = model.predict_classes(X_test) \n",
        "        y_test = [int(i) for i in y_test]\n",
        "        print(metrics.classification_report(y_test, y_pred))\n",
        "        results = [x + y for x, y in zip(results , list( metrics.precision_recall_fscore_support(y_test, y_pred, average=\"macro\")[:-1] + metrics.precision_recall_fscore_support(y_test, y_pred, average=\"weighted\")[:-1]))]\n",
        "        break\n",
        "    print([round(x / 5, 2) for x in results ])\n",
        "print(\"LSTM Classify\")\n",
        "for i in range (5,20):\n",
        "  EPOCHS = i\n",
        "  classify(X, Y)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTM Classify\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 15860 samples, validate on 3966 samples\n",
            "Epoch 1/5\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "15860/15860 [==============================] - 32s 2ms/step - loss: 0.5545 - acc: 0.8084 - val_loss: 0.3459 - val_acc: 0.8901\n",
            "Epoch 2/5\n",
            "15860/15860 [==============================] - 31s 2ms/step - loss: 0.3353 - acc: 0.8956 - val_loss: 0.3121 - val_acc: 0.8986\n",
            "Epoch 3/5\n",
            "15860/15860 [==============================] - 31s 2ms/step - loss: 0.2640 - acc: 0.9168 - val_loss: 0.3089 - val_acc: 0.8981\n",
            "Epoch 4/5\n",
            "15860/15860 [==============================] - 31s 2ms/step - loss: 0.2009 - acc: 0.9334 - val_loss: 0.3485 - val_acc: 0.8981\n",
            "Epoch 5/5\n",
            "15860/15860 [==============================] - 32s 2ms/step - loss: 0.1541 - acc: 0.9511 - val_loss: 0.3911 - val_acc: 0.8901\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.30      0.19      0.23       286\n",
            "           1       0.91      0.94      0.92      3838\n",
            "           2       0.79      0.77      0.78       833\n",
            "\n",
            "    accuracy                           0.87      4957\n",
            "   macro avg       0.67      0.63      0.64      4957\n",
            "weighted avg       0.85      0.87      0.86      4957\n",
            "\n",
            "[0.13, 0.13, 0.13, 0.17, 0.17, 0.17]\n",
            "Train on 15860 samples, validate on 3966 samples\n",
            "Epoch 1/6\n",
            "15860/15860 [==============================] - 32s 2ms/step - loss: 0.5578 - acc: 0.8140 - val_loss: 0.3303 - val_acc: 0.8964\n",
            "Epoch 2/6\n",
            "15860/15860 [==============================] - 31s 2ms/step - loss: 0.3262 - acc: 0.9010 - val_loss: 0.3115 - val_acc: 0.8999\n",
            "Epoch 3/6\n",
            "15860/15860 [==============================] - 31s 2ms/step - loss: 0.2575 - acc: 0.9199 - val_loss: 0.3125 - val_acc: 0.9004\n",
            "Epoch 4/6\n",
            "15860/15860 [==============================] - 31s 2ms/step - loss: 0.1999 - acc: 0.9306 - val_loss: 0.3291 - val_acc: 0.8991\n",
            "Epoch 5/6\n",
            "15860/15860 [==============================] - 31s 2ms/step - loss: 0.1531 - acc: 0.9485 - val_loss: 0.3640 - val_acc: 0.8944\n",
            "Epoch 6/6\n",
            " 2460/15860 [===>..........................] - ETA: 25s - loss: 0.1209 - acc: 0.9634"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-785e88986449>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m   \u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-785e88986449>\u001b[0m in \u001b[0;36mclassify\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOqxRKBQWp_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn import pipeline, feature_extraction, svm, metrics\n",
        "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
        "def CNN_model():\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(vocab_size, 100, input_length=max_document_length))\n",
        "        model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(MaxPooling1D(4))\n",
        "        model.add(Conv1D(filters=128, kernel_size=5,  activation='relu'))\n",
        "        model.add(GlobalAveragePooling1D())\n",
        "        # model.add(LSTM(10))\n",
        "        # model.add(Dense(3, activation='softmax'))\n",
        "        # model.add(Dropout(0.5))\n",
        "        model.add(Dense(3, activation=\"softmax\"))\n",
        "        # model.add(Activation('softmax'))\n",
        "        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "def classify(X, y):\n",
        "    # self.split(X,y)\n",
        "    cv = StratifiedKFold(n_splits=5)\n",
        "    results = [0,0,0, 0,0,0,0,0,0]\n",
        "    for train_idx, test_idx, in cv.split(X, y):\n",
        "        X_train, y_train = X[train_idx], y[train_idx]\n",
        "        X_test, y_test = X[test_idx], y[test_idx]\n",
        "        model = CNN_model()\n",
        "        model.fit(X_train, y_train, validation_split=0.25, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1)\n",
        "\n",
        "        y_pred = model.predict_classes(X_test)\n",
        "        y_test = [int(i) for i in y_test]\n",
        "        print(metrics.classification_report(y_test, y_pred))\n",
        "        results = [x + y for x, y in zip(results , list( metrics.precision_recall_fscore_support(y_test, y_pred, average=\"macro\")[:-1] + metrics.precision_recall_fscore_support(y_test, y_pred, average=\"weighted\")[:-1]))]\n",
        "        break\n",
        "    print([round(x , 2) for x in results ])\n",
        "for i in range (5,20，3):\n",
        "  EPOCHS = i\n",
        "  classify(X, Y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFspqQv7A9Ud",
        "colab_type": "code",
        "outputId": "12a9aee7-3d72-4342-efdd-dba95c14f7ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn import pipeline, feature_extraction, svm, metrics\n",
        "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
        "def CNN_LSTM_model():\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(vocab_size, 100, input_length=max_document_length))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(Conv1D(filters=100, kernel_size=4, activation='linear'))\n",
        "        # model.add(Dropout(0.7))\n",
        "        model.add(MaxPooling1D(4))\n",
        "        model.add(LSTM(100, return_sequences = True))\n",
        "      \n",
        "        model.add(GlobalAveragePooling1D(data_format='channels_last'))\n",
        "        # model.add(LSTM(10))\n",
        "        # model.add(Dense(3, activation='softmax'))\n",
        "        # model.add(Dropout(0.5))\n",
        "       \n",
        "        model.add(Dense(3, activation=\"softmax\"))\n",
        "        \n",
        "        # model.add(Activation('softmax'))\n",
        "        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    \n",
        "        return model\n",
        "\n",
        "def classify(X, y):\n",
        "    # self.split(X,y)\n",
        "    cv = StratifiedKFold(n_splits=5)\n",
        "    results = [0,0,0, 0,0,0,0,0,0]\n",
        "    for train_idx, test_idx, in cv.split(X, y):\n",
        "        X_train, y_train = X[train_idx], y[train_idx]\n",
        "        X_test, y_test = X[test_idx], y[test_idx]\n",
        "        model = CNN_LSTM_model()\n",
        "        model.fit(X_train, y_train, validation_split=0.25, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1)\n",
        "\n",
        "        y_pred = model.predict_classes(X_test)\n",
        "        y_test = [int(i) for i in y_test]\n",
        "        print(metrics.classification_report(y_test, y_pred))\n",
        "        results = [x + y for x, y in zip(results , list( metrics.precision_recall_fscore_support(y_test, y_pred, average=\"macro\")[:-1] + metrics.precision_recall_fscore_support(y_test, y_pred, average=\"weighted\")[:-1]))]\n",
        "        break\n",
        "    print([round(x , 2) for x in results ])\n",
        "for i in range (5, 20, 3):\n",
        "  EPOCHS = i\n",
        "  classify(X, Y)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 14869 samples, validate on 4957 samples\n",
            "Epoch 1/5\n",
            "14869/14869 [==============================] - 32s 2ms/step - loss: 0.4863 - acc: 0.8340 - val_loss: 0.2892 - val_acc: 0.9112\n",
            "Epoch 2/5\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.2791 - acc: 0.9106 - val_loss: 0.2796 - val_acc: 0.9135\n",
            "Epoch 3/5\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.2040 - acc: 0.9309 - val_loss: 0.3055 - val_acc: 0.9020\n",
            "Epoch 4/5\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.1318 - acc: 0.9562 - val_loss: 0.3449 - val_acc: 0.8882\n",
            "Epoch 5/5\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.0765 - acc: 0.9780 - val_loss: 0.3705 - val_acc: 0.8901\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.34      0.34       286\n",
            "           1       0.91      0.92      0.92      3838\n",
            "           2       0.81      0.77      0.79       833\n",
            "\n",
            "    accuracy                           0.86      4957\n",
            "   macro avg       0.68      0.68      0.68      4957\n",
            "weighted avg       0.86      0.86      0.86      4957\n",
            "\n",
            "[0.68, 0.68, 0.68, 0.86, 0.86, 0.86]\n",
            "Train on 14869 samples, validate on 4957 samples\n",
            "Epoch 1/8\n",
            "14869/14869 [==============================] - 33s 2ms/step - loss: 0.4662 - acc: 0.8414 - val_loss: 0.2868 - val_acc: 0.9155\n",
            "Epoch 2/8\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.2743 - acc: 0.9109 - val_loss: 0.2751 - val_acc: 0.9139\n",
            "Epoch 3/8\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.1907 - acc: 0.9373 - val_loss: 0.2802 - val_acc: 0.9122\n",
            "Epoch 4/8\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.1068 - acc: 0.9654 - val_loss: 0.3273 - val_acc: 0.8959\n",
            "Epoch 5/8\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.0596 - acc: 0.9826 - val_loss: 0.3600 - val_acc: 0.8945\n",
            "Epoch 6/8\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.0358 - acc: 0.9895 - val_loss: 0.4206 - val_acc: 0.8826\n",
            "Epoch 7/8\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.0264 - acc: 0.9927 - val_loss: 0.4386 - val_acc: 0.8862\n",
            "Epoch 8/8\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.0178 - acc: 0.9954 - val_loss: 0.4624 - val_acc: 0.8725\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.26      0.35      0.30       286\n",
            "           1       0.91      0.89      0.90      3838\n",
            "           2       0.77      0.78      0.78       833\n",
            "\n",
            "    accuracy                           0.84      4957\n",
            "   macro avg       0.65      0.67      0.66      4957\n",
            "weighted avg       0.85      0.84      0.85      4957\n",
            "\n",
            "[0.65, 0.67, 0.66, 0.85, 0.84, 0.85]\n",
            "Train on 14869 samples, validate on 4957 samples\n",
            "Epoch 1/11\n",
            "14869/14869 [==============================] - 33s 2ms/step - loss: 0.4672 - acc: 0.8390 - val_loss: 0.3043 - val_acc: 0.9118\n",
            "Epoch 2/11\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.2779 - acc: 0.9109 - val_loss: 0.2832 - val_acc: 0.9106\n",
            "Epoch 3/11\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.2134 - acc: 0.9278 - val_loss: 0.3257 - val_acc: 0.9005\n",
            "Epoch 4/11\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.1531 - acc: 0.9481 - val_loss: 0.3314 - val_acc: 0.8943\n",
            "Epoch 5/11\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.0975 - acc: 0.9695 - val_loss: 0.3659 - val_acc: 0.8899\n",
            "Epoch 6/11\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.0570 - acc: 0.9844 - val_loss: 0.3937 - val_acc: 0.8963\n",
            "Epoch 7/11\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.0381 - acc: 0.9901 - val_loss: 0.4394 - val_acc: 0.8864\n",
            "Epoch 8/11\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.0239 - acc: 0.9936 - val_loss: 0.4707 - val_acc: 0.8852\n",
            "Epoch 9/11\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.0186 - acc: 0.9943 - val_loss: 0.4932 - val_acc: 0.8759\n",
            "Epoch 10/11\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.0143 - acc: 0.9962 - val_loss: 0.5532 - val_acc: 0.8828\n",
            "Epoch 11/11\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.0109 - acc: 0.9966 - val_loss: 0.5607 - val_acc: 0.8727\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.23      0.36      0.28       286\n",
            "           1       0.92      0.89      0.91      3838\n",
            "           2       0.81      0.74      0.77       833\n",
            "\n",
            "    accuracy                           0.84      4957\n",
            "   macro avg       0.65      0.67      0.65      4957\n",
            "weighted avg       0.86      0.84      0.85      4957\n",
            "\n",
            "[0.65, 0.67, 0.65, 0.86, 0.84, 0.85]\n",
            "Train on 14869 samples, validate on 4957 samples\n",
            "Epoch 1/14\n",
            "14869/14869 [==============================] - 32s 2ms/step - loss: 0.4735 - acc: 0.8412 - val_loss: 0.3075 - val_acc: 0.9104\n",
            "Epoch 2/14\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.2790 - acc: 0.9108 - val_loss: 0.2789 - val_acc: 0.9139\n",
            "Epoch 3/14\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.2051 - acc: 0.9311 - val_loss: 0.3032 - val_acc: 0.9090\n",
            "Epoch 4/14\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.1321 - acc: 0.9568 - val_loss: 0.3188 - val_acc: 0.8975\n",
            "Epoch 5/14\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.0753 - acc: 0.9779 - val_loss: 0.3447 - val_acc: 0.8963\n",
            "Epoch 6/14\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.0455 - acc: 0.9874 - val_loss: 0.3907 - val_acc: 0.8943\n",
            "Epoch 7/14\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.0307 - acc: 0.9916 - val_loss: 0.4292 - val_acc: 0.8838\n",
            "Epoch 8/14\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.0221 - acc: 0.9939 - val_loss: 0.4562 - val_acc: 0.8832\n",
            "Epoch 9/14\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.0163 - acc: 0.9956 - val_loss: 0.5061 - val_acc: 0.8794\n",
            "Epoch 10/14\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.0143 - acc: 0.9962 - val_loss: 0.5149 - val_acc: 0.8743\n",
            "Epoch 11/14\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.0108 - acc: 0.9972 - val_loss: 0.5785 - val_acc: 0.8636\n",
            "Epoch 12/14\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.0091 - acc: 0.9974 - val_loss: 0.5510 - val_acc: 0.8830\n",
            "Epoch 13/14\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.0082 - acc: 0.9975 - val_loss: 0.6395 - val_acc: 0.8723\n",
            "Epoch 14/14\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.0075 - acc: 0.9975 - val_loss: 0.6002 - val_acc: 0.8790\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.29      0.40      0.33       286\n",
            "           1       0.92      0.89      0.90      3838\n",
            "           2       0.76      0.78      0.77       833\n",
            "\n",
            "    accuracy                           0.84      4957\n",
            "   macro avg       0.65      0.69      0.67      4957\n",
            "weighted avg       0.85      0.84      0.85      4957\n",
            "\n",
            "[0.65, 0.69, 0.67, 0.85, 0.84, 0.85]\n",
            "Train on 14869 samples, validate on 4957 samples\n",
            "Epoch 1/17\n",
            "14869/14869 [==============================] - 33s 2ms/step - loss: 0.4566 - acc: 0.8444 - val_loss: 0.2806 - val_acc: 0.9169\n",
            "Epoch 2/17\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.2765 - acc: 0.9117 - val_loss: 0.2830 - val_acc: 0.9120\n",
            "Epoch 3/17\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.2115 - acc: 0.9282 - val_loss: 0.3084 - val_acc: 0.9040\n",
            "Epoch 4/17\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.1379 - acc: 0.9529 - val_loss: 0.3429 - val_acc: 0.8983\n",
            "Epoch 5/17\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.0781 - acc: 0.9759 - val_loss: 0.3626 - val_acc: 0.8929\n",
            "Epoch 6/17\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.0465 - acc: 0.9876 - val_loss: 0.4000 - val_acc: 0.8854\n",
            "Epoch 7/17\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.0297 - acc: 0.9921 - val_loss: 0.4344 - val_acc: 0.8830\n",
            "Epoch 8/17\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.0204 - acc: 0.9951 - val_loss: 0.4715 - val_acc: 0.8810\n",
            "Epoch 9/17\n",
            "14869/14869 [==============================] - 30s 2ms/step - loss: 0.0162 - acc: 0.9955 - val_loss: 0.5368 - val_acc: 0.8705\n",
            "Epoch 10/17\n",
            " 7710/14869 [==============>...............] - ETA: 14s - loss: 0.0120 - acc: 0.9969"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpUcPeM0V-JW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cp_hLs-NKhH8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn import pipeline, feature_extraction, svm, metrics\n",
        "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "def CNN_LSTM_model():\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(vocab_size, 100, input_length=max_document_length))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(Conv1D(filters=100, kernel_size=4, activation='linear'))\n",
        "        # model.add(Dropout(0.7))\n",
        "        model.add(MaxPooling1D(4))\n",
        "        model.add(LSTM(100, return_sequences = True))\n",
        "      \n",
        "        model.add(GlobalAveragePooling1D(data_format='channels_last'))\n",
        "        # model.add(LSTM(10))\n",
        "        # model.add(Dense(3, activation='softmax'))\n",
        "        # model.add(Dropout(0.5))\n",
        "       \n",
        "        model.add(Dense(3, activation=\"softmax\"))\n",
        "        \n",
        "        # model.add(Activation('softmax'))\n",
        "        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    \n",
        "        return model\n",
        "\n",
        "def classify(X, y):\n",
        "    # self.split(X,y)\n",
        "    cv = StratifiedKFold(n_splits=5)\n",
        "    results = [0,0,0, 0,0,0,0,0,0]\n",
        "    for train_idx, test_idx, in cv.split(X, y):\n",
        "        X_train, y_train = X[train_idx], y[train_idx]\n",
        "        X_test, y_test = X[test_idx], y[test_idx]\n",
        "        model = CNN_LSTM_model()\n",
        "        model.fit(X_train, y_train, validation_split=0.25, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1)\n",
        "\n",
        "        y_pred = model.predict_classes(X_test)\n",
        "        y_test = [int(i) for i in y_test]\n",
        "        print(metrics.classification_report(y_test, y_pred))\n",
        "        results = [x + y for x, y in zip(results , list( metrics.precision_recall_fscore_support(y_test, y_pred, average=\"macro\")[:-1] + metrics.precision_recall_fscore_support(y_test, y_pred, average=\"weighted\")[:-1]))]\n",
        "        break\n",
        "    print([round(x , 2) for x in results ])\n",
        "for i in range (5, 20, 3):\n",
        "  EPOCHS = i\n",
        "  classify(X, Y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}